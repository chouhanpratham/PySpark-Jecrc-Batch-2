{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75276e17-6e90-46eb-9291-2cb144a90bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">List(10, 20, 30)\n",
       "+---+-----+------+\n",
       " id| name|gender|\n",
       "+---+-----+------+\n",
       "  1|Alice|Female|\n",
       "  2|  Bob|  Male|\n",
       "+---+-----+------+\n",
       "\n",
       "nums: List[Int] = List(10, 20, 30)\n",
       "data: Seq[(Int, String, String)] = List((1,Alice,Female), (2,Bob,Male))\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">List(10, 20, 30)\n+---+-----+------+\n| id| name|gender|\n+---+-----+------+\n|  1|Alice|Female|\n|  2|  Bob|  Male|\n+---+-----+------+\n\nnums: List[Int] = List(10, 20, 30)\ndata: Seq[(Int, String, String)] = List((1,Alice,Female), (2,Bob,Male))\ndf: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n</div>",
       "datasetInfos": [
        {
         "name": "df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "gender",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// Declare an immutable list  \n",
    "val nums = List(10, 20, 30)  \n",
    "println(nums) // Output: List(10, 20, 30)  \n",
    "\n",
    "// Create a sequence of tuples and convert to DataFrame  \n",
    "val data = Seq((1, \"Alice\", \"Female\"), (2, \"Bob\", \"Male\"))  \n",
    "val df = spark.createDataFrame(data).toDF(\"id\", \"name\", \"gender\")  \n",
    "df.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdb6522-8a98-4586-a2dc-6ba3b0f2e1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+------+---------+--------+-------+\n",
       "      date|region|  product|quantity|revenue|\n",
       "+----------+------+---------+--------+-------+\n",
       "2024-01-01| North|Product A|      10|  200.0|\n",
       "2024-01-01| South|Product B|       5|  300.0|\n",
       "2024-01-02| North|Product A|      20|  400.0|\n",
       "2024-01-02| South|Product B|      10|  600.0|\n",
       "2024-01-03|  East|Product C|      15|  375.0|\n",
       "+----------+------+---------+--------+-------+\n",
       "\n",
       "salesData: Seq[(String, String, String, Int, Double)] = List((2024-01-01,North,Product A,10,200.0), (2024-01-01,South,Product B,5,300.0), (2024-01-02,North,Product A,20,400.0), (2024-01-02,South,Product B,10,600.0), (2024-01-03,East,Product C,15,375.0))\n",
       "df: org.apache.spark.sql.DataFrame = [date: string, region: string ... 3 more fields]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------+------+---------+--------+-------+\n|      date|region|  product|quantity|revenue|\n+----------+------+---------+--------+-------+\n|2024-01-01| North|Product A|      10|  200.0|\n|2024-01-01| South|Product B|       5|  300.0|\n|2024-01-02| North|Product A|      20|  400.0|\n|2024-01-02| South|Product B|      10|  600.0|\n|2024-01-03|  East|Product C|      15|  375.0|\n+----------+------+---------+--------+-------+\n\nsalesData: Seq[(String, String, String, Int, Double)] = List((2024-01-01,North,Product A,10,200.0), (2024-01-01,South,Product B,5,300.0), (2024-01-02,North,Product A,20,400.0), (2024-01-02,South,Product B,10,600.0), (2024-01-03,East,Product C,15,375.0))\ndf: org.apache.spark.sql.DataFrame = [date: string, region: string ... 3 more fields]\n</div>",
       "datasetInfos": [
        {
         "name": "df",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "date",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "region",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "product",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "quantity",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "revenue",
            "nullable": false,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val salesData = Seq(\n",
    "  (\"2024-01-01\", \"North\", \"Product A\", 10, 200.0),\n",
    "  (\"2024-01-01\", \"South\", \"Product B\", 5, 300.0),\n",
    "  (\"2024-01-02\", \"North\", \"Product A\", 20, 400.0),\n",
    "  (\"2024-01-02\", \"South\", \"Product B\", 10, 600.0),\n",
    "  (\"2024-01-03\", \"East\",  \"Product C\", 15, 375.0)\n",
    ")\n",
    "val df = spark.createDataFrame(salesData).toDF(\"date\", \"region\", \"product\", \"quantity\", \"revenue\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "376e8bd9-a466-4eb9-9cd6-76a1bb7e8a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Total Revenue for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9404b4cf-5d09-48c2-98cc-f79364e85817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5 Scala Demo 2025-04-24 12:08:31",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}